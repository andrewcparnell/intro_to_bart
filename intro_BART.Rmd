---
title: "Introduction to Bayesian Additive Regression Trees"
date: "5th December, 2018"
author: 
  - "Andrew C Parnell"
output:
  xaringan::moon_reader:
    css: [default, maynooth, maynooth-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
library(tidyverse)

```

class: big

# Goals

- Teach my PhD students the basics of BART

- Get the other Hamiltonians interested in collaborating on BART

- Get the statistics group up to speed on where they can work on BART

---

# What is BART?

```{r, echo = FALSE, out.height="200", fig.align='default'}
myimages<- c("https://img.huffingtonpost.com/asset/579f38881200007404a54711.jpeg",
             "https://media.ktvu.com/media.ktvu.com/photo/2015/07/31/28934772_6282569_G_70469_ver1.0_640_360.jpg")
knitr::include_graphics(myimages)
```

***

$$\Huge y = \sum_{j=1}^M g(X, T_j, \Theta_j, \mu_j) + \epsilon$$

Chipman, Hugh A., Edward I. George, and Robert E. McCulloch. BART: Bayesian additive regression trees. _The Annals of Applied Statistics 4_, no. 1 (2010): 266-298.

---

# BART works (part 1)

```{r}
data(BostonHousing, package = "mlbench")
BostonHousing %>% head
```

---

# BART works (part 1)

```{r, echo = FALSE, out.height=500}
myimages<- "Boston_housing_benchmark.jpg"
knitr::include_graphics(myimages)
```
---

# BART works (part 2)

A commonly used method for simulating complex regression-type data is:

$$y = \beta_1 \sin(\pi x_1 x_2) + \beta_2 (x_3 - 0.5)^2 + \beta_3 x_4 + \beta_5 x_5 + \epsilon; \; \epsilon \sim N(0, 1)$$

where $x_j \sim N(0, 1)$ for $j=1, \ldots, 5$

From the Freidman Multivariate Adaptive Regression Splines paper of 1991

Task is to create good predictions of $y$ and identify which variables are important

```{r, echo = FALSE}
source('https://raw.githubusercontent.com/andrewcparnell/rBART/master/rBART.R')
# Now do it on the Friedman example
```

```{r}
set.seed(123)
dat = sim_friedman(500)
fried = data.frame(dat$X, y = dat$y)
head(fried)
```

---
  
# BART works (part 2)

```{r, echo = FALSE, out.height=500}
myimages<- "Friedman_benchmark.jpg"
knitr::include_graphics(myimages)
```

---

class: big

# Key BART advantages

- Very good prediction properties

- Deals with interactions and non-linear relationships

- Based on a probability distribution so 'easily' extendable

- A Bayesian model so have probabilistic uncertainty intervals for any required quantities

---

# BART prediction intervals

```{r, echo = FALSE, out.height=500}
myimages<- "BART_prediction_intervals.jpg"
knitr::include_graphics(myimages)
```

---

# Back to ba(yes)-sics

- Multiple linear regression:

$$y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon,\; \epsilon \sim N(0, \sigma^2)$$

- In Bayesian  world we try to find the _posterior probability distribution_ of the parameters $(\sigma, \alpha, \beta_1, \ldots, \beta_p)$ given the data $(y \mbox{ and } x_1, x_2, \ldots, x_p)$

- We give _prior distributions_ to the parameters to constrain them to realistic values in the posterior

- The model fitting proceeds by creating a sequential algorithm called a Gibbs sampler where we update each parameter individually _given all the parameters and data_ using an accept/reject algorithm

- The algorithm takes into account the likelihood (here the linear model structure) and the prior probability distributions

---

# Gibbs sampling

![Hamiltonian Monte Carlo in action](https://raw.githubusercontent.com/chi-feng/mcmc-demo/master/docs/rwmh.gif)

---

# Convergence

```{r, echo = FALSE, fig.align="center", out.width = "\\maxwidth"}
df = data.frame(iter = 1:1000,
                alpha = rnorm(1000, 3, 2),
                beta = rnorm(1000, -3, 0.1),
                sigma = rgamma(1000, 1, 1))
df2 = df %>% gather(-iter, 
                    key = variable,
                    value = value)
ggplot(df2, aes(x = iter, y = value, colour = variable)) + 
  geom_line() + 
  theme_bw() + 
  facet_grid(variable ~ ., scales = 'free_y') + 
  theme(legend.position = 'None') + 
  labs(x = 'Iteration',
       y = 'Paramter value')
```

---

# Regression trees

```{r, echo = FALSE, out.height=500}
myimages<- "tree_58.jpg"
knitr::include_graphics(myimages)
```

---

# Regression trees in code

```{r, echo = FALSE}
tree_58 = readRDS('tree_58.rds')
tree_58 %>% round(., digits = 3)
```

---

class: big

# Regression trees and Bayes

- We still have $X$ and $y$ as our data

- Our new parameters are the tree structure (discrete), the split variables (discrete), the split values (continuous) and the terminal node values (continuous)

- For given parameters and data we can create predictions

- We use MCMC exactly as before to create a posterior distribution of the parameters given the data

---

class: big

# A single regression tree model

$$\Large y = g(X, T, \Theta, \mu) + \epsilon; \epsilon \sim N(0, \sigma^2)$$

- Here $y$ is the response and $X$ the covariates (data)
- $T$ is a tree structure (parameter)
- $\Theta$ is a set of split variables and values (parameters)
- $\mu$ is the set of terminal node values (parameters)
- The algorithm works by guessing initial values of all these (usually a stump), then proposing and accepting/rejecting new trees
- The big problem that occurs here is that the number of parameters changes when the tree structure changes

---

# Collapsing over trees

- The main reason that BART works is that we can collapse over the tree structure when we use a normally distributed prior on the $\mu$ terminal node parameters. 

- Suppose $y_1, \ldots, y_n$ are all the observations that fall into a single terminal node
- Let $\tau = \sigma^-2$ and suppose the mean parameter in this terminal node has prior $\mu \sim N(\mu_\mu, \tau_\mu^{-1})$.

- Then: 
  
\begin{eqnarray*}
\pi(y_1, \ldots, y_n| \sigma, \ldots) &=& \int \pi(y_1, \ldots, y_n| \mu, \sigma) \pi(\mu) \partial \mu \\
&=&  \tau^{n/2} \left( \frac{\tau_\mu}{\tau_\mu + n \tau} \right)^{1/2} \exp \left[ -\frac{\tau}{2} \left\{ \sum y_i^2 - \frac{ \tau (n\bar{y})^2 }{ \tau_\mu + n \tau } \right\} \right]
\end{eqnarray*}

- Can extend this to multiple terminal nodes for a single tree via products

---

class: big

# A sum of trees model

$$\Large y = \sum_{j=1}^M g(X, T_j, \Theta_j, \mu_j) + \epsilon;\; \epsilon \sim N(0, \sigma^2)$$

- Standard BART MCMC uses a back-fitting approach where each tree is fixed and the others updated in turn

- Requires us to propose and accept/reject a new tree at each iteration for $j=1, \ldots, M$ trees

- We put extra prior distributions on the size and shape of the trees to keep them small, and to keep the $\mu$ values from dominating the predictions


---

# Grow, prune, swap, and change

- Default BART has four moves to generate new trees, all of which are reversible

- __Grow__ picks a random terminal node and splits it in two by choosing a random (uniform) split variable and split value

- __Prune__ picks a pair of adjacent terminal nodes and merges them

- __Swap__ picks two random (uniform) internal nodes and swaps their split variables and values

- __Change__ picks a random (uniform) internal node and changes the split variable and value

- Others have been proposed over the years but haven't yielded large performance improvements

---

# The BART algorithm

```
Data: Target variable y (length n), 
      Feature matrix X (n rows and p columns)
Result: Posterior list of trees T, values of tau, fitted values y_hat
Intitialisation: 
  Number of trees M
	Number of iterations N
	Initial value tau = 1
	Set trees T_j; j = 1, ..., M to stumps
	Set values of mu to 0
For iterations i from 1 to N:
	For trees j from 1 to M:
		Compute partial residuals from y minus predictions of all trees except tree j
		Grow a new tree T_j_new based on grow/prune/change/swap
		Set l_new = log full conditional of new tree T_j_new
		Set l_old = log full conditional of old tree T_j
		Set a = exp(l_new - l_old)
		Generate u from U(0, 1)
		If u < a:
			Set T_j = T_j_new
		Simulate mu values for T_j
  End tree loop
  Get predictions y_hat from all trees
	Update tau
End iteration loop
```

---

# Problems with convergence

- It's very hard to monitor convergence with varying dimensional parameters

- You can store the full conditionals of the different trees, or plot the posterior predictive likelihood

- However a small change in the tree can wreck the convergence properties

```{r, echo = FALSE, fig.height = 4}
sigma_post = readRDS('rBART_sigma.rds')
df = data.frame(Iteration = 1:length(sigma_post),
                Sigma = sigma_post)
ggplot(df, aes(x = Iteration, y = Sigma)) + 
  geom_line() + 
  theme_bw() + 
  ggtitle('10000 iterations (removing 2000 warm up and thinning by 8)')
```

---

class: big

# Extensions of BART

- Because BART is built on a standard set of probability distributions it's quite easily extendable to other types of models _provided you can still perform the collapsing over the terminal node parameters_

- Some extensions have already been done:

    - 2 and multi-class classification models
    - Multivariate models
    - Monotone response models
    - Survival analysis models

---

class: big

# Major problems with BART

- The fitting algorithm is the main bottleneck

- Choosing split variables and values uniformly is very far from optimal. Need a far greedier algorithm

- Better ways to explore trees?

- Lots of modelling types still not covered

---

class: big

# Summary

- BART is a Bayesian model with both great practical use and theoretical properties

- Has all the benefits of Bayes (prior information, good calibration properties), plus all the benefits of machine learning

- Needs more work on the algorithm to make it faster for bigger data sets, and more bespoke versions of it for specific sort of data

---

class: center, middle

# Questions?

